<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">
  <title>V-Soner</title>
  <script src="js/config.js"></script>
  <script src="js/chat_vision_api.js"></script>
  <script src="js/ui_helper.js"></script>
  <script src="js/speech_api.js"></script>
</head>

<body>
  <h1>Visual Sonar</h1>
  <p>背面カメラの映像を表示して、内容を説明します</p>
  <!--
  api key <input type="text" id="api_key_input" /><br />
  -->

  <button id="start_button" onclick="startCamera()">Start</button>
  &nbsp;
  <button id="stop_button" onclick="stopCamera()">Stop</button>
  &nbsp;&nbsp;
  <button id="take_photo_button" onclick="takePhoto()">Take Photo</button>
  &nbsp;
  <button id="explain_image_button" onclick="explainImage()">Explain</button>
  &nbsp;
  <button id="explain_voice_button" onclick="explainInVoice()">Voice</button>
  <br />

  <!--
<video width="640" height="480" autoplay playsinline style="border: 1px gray solid; width:320px; height: 240px;"></video>
-->
  <video width="640" height="960" autoplay playsinline
    style="border: 1px gray solid; width:320px; height: 480px;"></video>
  <br />
  <div id="explain_result" style="border: 1px gray solid; width:320px; height:240px; font-size: small;"></div>
  <br />
  <audio id="playback_audio"></audio>
  <br />
  <img id="photo" src="" style="border: 1px blue solid;">


</body>
<script>
  const localVideo = document.querySelector('video');
  const photo = document.getElementById('photo');
  const divExplain = document.getElementById('explain_result');
  const playbackAudio = document.getElementById('playback_audio');

  let localStream = null;
  let workCanvas = null;
  let workCtx = null;

  async function startCamera() {
    try {
      const constraints = {
        video: {
          //facingMode: "user" // フロントカメラを使用
          facingMode: "environment" // 背面カメラを使用
        }
      };

      // カメラの映像を取得
      const stream = await navigator.mediaDevices.getUserMedia(constraints);

      // <video> エレメントにストリームを設定
      localVideo.srcObject = stream;
      await localVideo.play();
    } catch (error) {
      console.error('カメラのアクセスに失敗しました:', error);
    }
  }

  function stopCamera() {
    if (localVideo.srcObject) {
      localVideo.pause();
      localVideo.srcObject.getTracks().forEach(track => {
        track.stop();
      });
      localVideo.srcObject = null;
    }
  }

  function takePhoto() {
    const dataURI = getBase64Image(localVideo);
    console.log(dataURI);
    photo.width = localVideo.videoWidth;
    photo.height = localVideo.videoHeight;
    photo.src = dataURI;
  }

  async function explainImage() {
    disableElementById('explain_image_button');
    const imageUrl = photo.src;

    console.log('====== start ask ======');
    //divExplain.innerText = '-- start explain --'
    divExplain.innerText = '--start-- ' + imageUrl;
    const response = await askAboutImage(imageUrl);

    console.log('====== answer ======');
    console.log(response.content);
    divExplain.innerText = response.content;
    enableElementById('explain_image_button');
  }

  async function explainInVoice() {
    const text = divExplain.innerText;
    disableElementById('explain_voice_button');

    const responseBlob = await textToSpeech(text, apiKey);
    if (responseBlob) {
      await playbacBlobAsync(playbackAudio, responseBlob);
    }
    enableElementById('explain_voice_button');
  }

  function getBase64Image(video) {
    // New Canvas
    if (!workCanvas) {
      //workCanvas = new OffscreenCanvas(video.videoWidth, video.videoHeight); // toDataUrl() not exist
      workCanvas = document.createElement("canvas");
      workCanvas.width = video.videoWidth;
      workCanvas.height = video.videoHeight;
      workCtx = workCanvas.getContext("2d");
    }

    // Draw Image
    workCtx.drawImage(video, 0, 0);

    // To Base64
    return workCanvas.toDataURL("image/jpeg");

    // result
    // data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAASABIAAD/4QB ... +JN3Z6fJ5jtYIskjTqWnRm5WNkb7qHndQB//9k=
  }

  // ==== global context ===
  let gptCtx = null;

  // ---- try gpt4-vision-preview ---
  async function askAboutImage(imageUrl) {
    // --- send image to GPT ---
    //const text = 'What is this? Please answer in Japanese.';
    const text = 'この画像に何が写っているか、目が不自由な人向けに詳しく教えてください。'
    const options = { temperature: 0, max_tokens: 1000 };
    const response = await singleChatWithImage(imageUrl, text, gptCtx, options);

    return response;
  }


  // --- initalize option ---
  function buildInitOptions() {
    const options = {};
    // if (API_URL && API_URL.length > 0) {
    //   options.url = API_URL;
    // }
    // if (MODEL_NAME && MODEL_NAME.length > 0) {
    //   options.model = MODEL_NAME;
    // }
    // if (SEND_TOKEN_LIMIT && SEND_TOKEN_LIMIT > 0) {
    //   options.sendTokenLimt = SEND_TOKEN_LIMIT;
    // }

    // options.url = '';
    options.model = 'gpt-4-vision-preview';
    options.sendTokenLimt = 7000;

    return options;
  }


  // --- get API key from URL --
  const apiKey = getParamFromQueryString('key');

  // --- init Gpt context ---
  const options = buildInitOptions();
  //console.log('option for INIT:', options)
  gptCtx = initChat(apiKey, options); // Chatを初期化し、GPTコンテキストを保持する


</script>

</html>